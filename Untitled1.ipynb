{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl1C3_xbVooQ",
        "outputId": "bcbd91a1-e426-417c-f039-0bc672b90ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['John', 'enjoys', 'playing', 'football', 'while', 'Mary', 'loves', 'reading', 'books', 'in', 'the', 'library', '.']\n",
            "\n",
            "After stopword removal (token/POS):\n",
            "[('John', 'PROPN'), ('enjoys', 'VERB'), ('playing', 'VERB'), ('football', 'NOUN'), ('Mary', 'PROPN'), ('loves', 'AUX'), ('reading', 'VERB'), ('books', 'NOUN'), ('library', 'NOUN')]\n",
            "\n",
            "Lemmatized + POS (only verbs & nouns):\n",
            "[('John', 'PROPN'), ('enjoy', 'VERB'), ('play', 'VERB'), ('football', 'NOUN'), ('Mary', 'PROPN'), ('read', 'VERB'), ('book', 'NOUN'), ('library', 'NOUN')]\n",
            "\n",
            "Final lemmas (verbs & nouns only):\n",
            "['John', 'enjoy', 'play', 'football', 'Mary', 'read', 'book', 'library']\n"
          ]
        }
      ],
      "source": [
        "# pip install spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# load small English model (includes tokenizer, POS tagger, lemmatizer, stopwords)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"John enjoys playing football while Mary loves reading books in the library.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# 1) Segment into tokens\n",
        "tokens = [t.text for t in doc]\n",
        "\n",
        "# 2) Remove stopwords (and non-alphabetic tokens for cleanliness)\n",
        "content_tokens = [t for t in doc if not t.is_stop and t.is_alpha]\n",
        "\n",
        "# 3) Lemmatize (no stemming)\n",
        "# 4) Keep only verbs and nouns (NOUN = common nouns, PROPN = proper nouns)\n",
        "keep_pos = {\"VERB\", \"NOUN\", \"PROPN\"}\n",
        "filtered_lemmas = [(t.lemma_, t.pos_) for t in content_tokens if t.pos_ in keep_pos]\n",
        "\n",
        "# Pretty prints\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "print(\"\\nAfter stopword removal (token/POS):\")\n",
        "print([(t.text, t.pos_) for t in content_tokens])\n",
        "print(\"\\nLemmatized + POS (only verbs & nouns):\")\n",
        "print(filtered_lemmas)\n",
        "\n",
        "# If you only want the lemmas (strings):\n",
        "final_lemmas_only = [lemma for lemma, pos in filtered_lemmas]\n",
        "print(\"\\nFinal lemmas (verbs & nouns only):\")\n",
        "print(final_lemmas_only)\n"
      ]
    }
  ]
}